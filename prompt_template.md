# LLM Output Evaluator â€“ Prompt Template

## ðŸŽ¯ Goal
To evaluate model outputs based on:
- User Query
- Retrieved Context
- Final Answer
- AnswerFound (true/false)

## ðŸ“Œ Evaluation Rules
- If answerFound = true â†’ Output must be fully supported by the retrieved context.
- If answerFound = false â†’ Output should reflect missing info in context.

## âœ… Output Format

```json
{
  "evaluation": "Correct | Partially Correct | Incorrect",
  "rationale": "Why the score was given"
}
